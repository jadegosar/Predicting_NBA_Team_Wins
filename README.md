# Predicting NBA Team Wins in 2021-22 season

## Introduction
This repository hosts a final group project for the Machine Learning class that I took in my undergrad education at Notre Dame where I studied analytics. All information included was current as of the project completion data which was before the 21-22 NBA season began. The motivation for this project stemmed from a NBA offseason that featured a massive wave of player movement through free-agent signings and trades which dramatically shifted team odds of winning a championship. Due to the drastically different NBA landscape for the 21-22 season, we wanted to explore the impact that individual trades and signings would have on a team's projected success for the season before any games were played.

Since player movement is such an integral contributor to success in the NBA, we wanted to explore the impact of free agency on team performance. With the landscape of the league drastically altered since the Milwaukee Bucks were crowned champions last season, we hoped to predict team wins using player data from updated rosters. By assessing the impact individual players have on a game’s outcome, we will be able to predict next season’s results beyond mere speculation. Additionally, with the start of the season less than a month away, our models will rely purely on data from the past two seasons. A two year window will enable us to add weight to recent performance while still accounting for players who missed last season with injuries. This will ultimately allow us to pull out individual-level statistics on players that were traded during free-agency to predict how much each will contribute to their new team.

To accomplish this we used data gathered from the nbaStatR package which contains a substantial amount of information on metrics such as blocks, turnovers, field goals attempted, field goals made, steals, and free throws attempted to name a few. We further manipulated the data to add metrics such as successful field goal percentage, successful three-point percentage, and free throw percentage. Ultimately, the outputs of our project will be a bagging model, a tuned xgboost model, as well as an individual player analysis that will allow us to draw conclusions on whether a traded player will have a positive or negative impact on their new team

## Data Description
For our dataset, we used the nbastatR package to pull NBA game logs from the 2019-2020 and 2020-2021 regular seasons. In order to avoid bias, specifically those introduced by the 2020 playoff bubble, we did not use playoff statistics in our models. The only clear weakness of our data structure was that it may undervalue teams with high draft picks. Since our predictions are based on data from previous seasons, teams with high impact rookies are likely to be more successful than our model suggests

Broken down by player statistics for each game, these two seasons provided 45,447 observations measured over 48 variables, such as outcome of the game, whether it was the second game of back-to-back, and offensive and defensive statistics for each player. We then combined this data into team statistics for each game, using a for loop to compile past averages. As a result, we were able to look at average offensive and defensive statistics over a specified period of time for each team prior to any game during the season. The new data frame consisted of 86 predictors, 43 for each team in the matchup. Each team’s first game in the specified range had to be omitted, however, as there was no previous data from which to calculate averages.

Since nbastatR is linked directly to the NBA Stats API, our dataset can be easily updated in real time by adjusting the dates from which data game logs are pulled. When this is done, the most recent game data will automatically be incorporated into the statistics used in our models. In our modeling process, we used 80% of our observations for training and 20% as a test set.

## Methods
The first model we ran was a random forest predicting game outcome by all of the quantitative variables in the dataset. We used 200 trees and an mtry of 84 for each of the variables in the game stat data frame. Using this model, we predicted outcomes, in terms of win or loss, for our test data set. Considering the frequency of uneven matchups in the NBA, in which game odds are not 50/50, the level of accuracy we obtained, at only .588, was not very impressive; however, we wanted to have results to compare our XGBoost model against when it comes to what variables have the most predictive power. To accomplish this, we extracted the importance of each variable. While it is possible that we could have tuned this model for a better result, we decided it was not powerful enough to confine our predictions to wins and losses.

To increase the power and applicability of our predictions, we ran an XGBoost model to predict game point differential. With the large dataset we used, employing XGBoost could give misclassified samples more weight, and in turn avoid errors on the test dataset. Since there are so many factors determining the outcome of an NBA game, we thought this would be advantageous. XGBoost is also flexible in terms of how it classifies and evaluates models. Since we switched from binary to continuous response, using XGBoost would allow us to switch back with relative ease.

The Random Forest we ran to make binary game outcome predictions displayed a 0.5907 accuracy when applied to our test data. Wins were set as the positive class, and the confusion matrix showed a sensitivity of 0.5872 and a specificity of .5940, making it fairly balanced but more likely to correctly predict a loss than a win. We ran an importance matrix to determine the most impactful variables in the Random Forest model, and we found that blocks were actually the most prominent predictor of wins, followed by average points scored of both teams. This was unexpected, but it suggests that the winningest teams over the past two seasons likely had strong interior defenses. It would be interesting to see if blocks remained a key predictor when more seasons are incorporated into the model. Overall, however, considering the uneven nature of many NBA matchups, predicting games at an accuracy of almost 59% was not especially impressive.

After creating the training and test matrices required by the XGBoost package, we then ran an initial model with nrounds set at 100. Since we were no longer running a binary model, root mean squared error was our evaluation metric, and this model returned an RMSE of 14.55. It appears, however, that this model overfit the data, as the training RMSE on round 100 was just 2.05. This disparity between training and test accuracy went away as we tuned our XGBoost hyperparameters

We tuned the model to find the optimal max depth, minimum child weight, gamma, subsample, colsample_bytree, and eta parameters. With a new RMSE of 14.46, the accuracy of the new model was slightly improved. Although this did not explicitly predict win or loss, the outcome of any game can be determined by whether the point differential was positive or negative. The tuned XGBoost model found points scored, as well as offensive and defensive field goal percentages to be especially predictive of point differential

## Results and Discussion
We used our XGBoost model to test the impact of player upgrades on the likelihood of winning. As an initial exploration, we substituted Russell Westbrook, one of the biggest acquisitions of the NBA offseason into the roster of his new team, the Los Angeles Lakers. Based on expected performance above replacement, we predict Westbrook to increase the winning percentage by 11-12%. Effectively, this is an improvement from 7th seed to 2nd seed in the Western Conference. Following a first round playoff exit, this is significant.

The main insights gained from our Random Forest and XGBoost model came from our feature importance analysis, in which we were able to pull out the variables in our dataset that impact the outcome of NBA games the most. In both models the most important predictors were blocks, average points scored for both teams, and a range of team defensive stats. Specifically, the Random Forest model showed defensive rebounds, points allowed, and field goal percentage against as powerful predictors for the binary outcome of win or loss. The fact that both of our models have similar variables with strong predicting power was promising in light of our accuracy and RMSE scores. Additionally, there were no variables in our XGBoost model that stood out from the rest in regards to the level of their importance. The feature with the most importance was points scored with other various point scoring measures following closely behind. Team rebounds and blocks also are important predictors with a value close to .2. All in all, the main insights we gained from our Random Forest and XGBoost model were what metrics in our dataset have the most predictive power when it comes to both game outcome and point difference. Although our models were not particularly accurate, the insights gained give us a better understanding of which aspects of NBA games are the most important for predicting a team’s success.

After creating our models to determine team success, we turned our attention to evaluating individual players in order to predict the impact that they will have on their new team. With this being the ultimate goal of our project, we decided to focus on one player in particular: Russell Westbrook. Russell Westbrook is possibly the biggest name traded to the Lakers this off-season and has a statistics line that led us to believe he will be a major asset to the Lakers this upcoming season. After replacing an average player, such as Kentavious Caldwell-Pope, with Westbrook and comparing how the Lakers would perform in each situation we came to the conclusion that Russell Westbrook will likely have a very large positive impact on the team’s success this upcoming season. To put it in perspective, we predicted that if the Lakers had Westbrook last year they would have won 57 games as opposed to 41 and would have a win percentage of 81%. Although this is a very high win percentage for any team to achieve, it is not far off to assume that the Super Team that is the Lakers this year could get close. Further action to be taken off this insight would be to do a similar analysis on other players and compare to the predictions we have made for Westbrook’s impact to gather a more comprehensive picture of how powerful the Lakers could be.

## Conclusion and Future Work
In conclusion, our report focuses on predicting team wins and point differential of games with the ultimate goal of being able to assess the likely impact a given traded player will have on their respective team’s success. In our analysis, we found that the most important predictors for our random forest model were team blocks, average points scored for both teams, and team defensive statistics such as field goal percentage against and points allowed. Our random forest model was focused on predicting team wins and losses and had an accuracy of .5872 which we decided was not powerful enough considering the difficulty that predicting wins in the NBA entails. When it comes to our XGBoost model, we focused on predicting point differential instead of a binary variable such as wins and losses due to the fact that it has more practical applications. In this case, our model had an RMSE of 14.46 after tuning the parameters and showed that average points scored, opponent team blocks, and team defensive statistics are the most important predictors when looking at point differential. Although both our models did not perform as well as we originally thought they would, we further researched what the typical accuracy score is of other models that have been created to predict NBA team wins and the upper bound ranges between 66-72% accuracy. We feel that with more time and resources we would be able to tune our model further to fully incorporate updated player rosters which would raise the accuracy of our model significantly going forward.

The key finding of the model that we created to predict an individual player’s impact on their team was that it seemed to be more accurate than the XGBoost model, although it is important to note that we were focused on point differential in our XGBoost model and win percentage in our player evaluation. We discovered this by running a player analysis on Russell Westbrook and predicting Laker wins if he had replaced KCP, an average player, last season. This model performed very well, predicting that the Lakers would win 41 games with KCP when they actually won 42. Furthermore, our model predicted win percentage in 2021 to be .5575 when it was actually .5915 which gave us the confidence to assume that running the model with Westbrook replacing KCP would show accurate results. Overall, the key takeaway from this analysis is that Russell Westbrook likely will have the largest positive impact of players traded to the Lakers and will significantly raise their chances of making a deep run this upcoming season. Given more time, we would be interested in applying the player model to all traded players in order to predict the ones that will have the most impact on their teams and attempt to factor in that particular ones may need their stats lowered to match new playing time expectations.

